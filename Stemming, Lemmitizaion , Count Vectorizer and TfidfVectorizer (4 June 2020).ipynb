{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Natural Language Tool Kit\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "# Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what’s often the simplest version of a\n",
    "#word. Connection, for example, would have the -ion suffix removed and be correctly reduced to connect. This kind of simple \n",
    "#stemming is often all that’s needed, but lemmatization—which actually looks at words and their roots (called lemma) as \n",
    "#described in the dictionary—is more precise (as long as the words exist in the dictionary).\n",
    "\n",
    "from nltk.stem import PorterStemmer #importing porter stemmer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['wait','waiting','waited','waits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating object of porter stemmer class\n",
    "port = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "# finding the root for each word in the list \"words\"\n",
    "for i in words:\n",
    "    root = port.stem(i)\n",
    "    print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We oberved above that we have a commnon root for all those words and that's what we wanted!\n",
    "# Now we'll see another example. Here we have a sentence and we first have to do the word tokenization and then stemming.\n",
    "\n",
    "text = \"Studying Studies Cries Cry\" # our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization = nltk.word_tokenize(text)  # doing word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for Studying is studi\n",
      "Stemming for Studies is studi\n",
      "Stemming for Cries is cri\n",
      "Stemming for Cry is cri\n"
     ]
    }
   ],
   "source": [
    "# printing the roots for each word/token generated in the sentence above. \n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the stemming above, we can see that the roots of the words generated are incorrect which prohibits us from using stemming\n",
    "# everywhere. Hence we use Lemmatization.\n",
    "#Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. aren’t \n",
    "#exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in \n",
    "#spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that\n",
    "#are forms of the word connect into the word connect itself.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  #importing Word Net Lemmatizer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating object\n",
    "word_net = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization for Studying is Studying\n",
      "Lemmatization for Studies is Studies\n",
      "Lemmatization for Cries is Cries\n",
      "Lemmatization for Cry is Cry\n"
     ]
    }
   ],
   "source": [
    "# applying the lemmatizer algorithm in the same text as above after tokenization\n",
    "for w in tokenization:\n",
    "    print(\"Lemmatization for {} is {}\".format(w,word_net.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The text must be parsed to remove words, called tokenization. Then the words need to be encoded as integers or floating point \n",
    "#values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n",
    "# COUNT VECTORIZATION = BAG OF WORDS\n",
    "# The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in\n",
    "#a document.This can be done by assigning each word a unique number. Then any document we see can be encoded as a fixed-length \n",
    "#vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count\n",
    "#or frequency of each word in the encoded document.This is the bag of words model, where we are only concerned with encoding \n",
    "#schemes that represent what words are present or the degree to which they are present in encoded documents without any \n",
    "#information about order.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer #importing our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words\n",
    "#, but also to encode new documents using that vocabulary. You can use it as follows:\n",
    "#Create an instance of the CountVectorizer class.\n",
    "#Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    "#Call the transform() function on one or more documents as needed to encode each as a vector.\n",
    "#Below is an example of using the CountVectorizer to tokenize, build a vocabulary, and then encode a document.\n",
    "tex1 = ['Natural Language Processing! This is used for text classification and sentiment analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating object/instance of CountVectorizer class\n",
    "vect = CountVectorizer()  # assign the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vect) #finding out the type of \"vect\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(tex1) #calling the fit() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 6,\n",
       " 'language': 5,\n",
       " 'processing': 7,\n",
       " 'this': 10,\n",
       " 'is': 4,\n",
       " 'used': 11,\n",
       " 'for': 3,\n",
       " 'text': 9,\n",
       " 'classification': 2,\n",
       " 'and': 1,\n",
       " 'sentiment': 8,\n",
       " 'analysis': 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_ #sorted alphabetically or we can say that we are seeing their column numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 6, 'language': 5, 'processing': 7, 'this': 10, 'is': 4, 'used': 11, 'for': 3, 'text': 9, 'classification': 2, 'and': 1, 'sentiment': 8, 'analysis': 0}\n"
     ]
    }
   ],
   "source": [
    "print(vect.vocabulary_) \n",
    "#We can see that all words were made lowercase by default and that the punctuation was ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vect.transform(tex1) # converting vect(object) to sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.toarray() # one row, 12 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One issue with simple counts is that some words like “the” will appear many times and their large counts will not be very \n",
    "# meaningful in the encoded vectors. That's why we use the far by most widely used method called \"TF-IDF\".\n",
    "# TFIDF VECTORIZATION\n",
    "# TF - Term frequency = (No of repitition of words in a sentence)/(no of words in a sentence)\n",
    "# IDF - Inverse Document frequency = log((no of sentences)/(no of sentence containing words))\n",
    "# We multiply TF with IDF to generate vector numbers and do the encoding afterwards\n",
    "# TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but\n",
    "# not across documents. The same create, fit, and transform process is used as with the CountVectorizer.\n",
    "# Below is an example of using the TfidfVectorizer to learn vocabulary and inverse document frequencies and then encode\n",
    "# those documents.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # importing our algorithm cum class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating oject/instance of the class\n",
    "vec1 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vec1) #finding out the type of \"vec1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.fit(tex1) #calling the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 6,\n",
       " 'language': 5,\n",
       " 'processing': 7,\n",
       " 'this': 10,\n",
       " 'is': 4,\n",
       " 'used': 11,\n",
       " 'for': 3,\n",
       " 'text': 9,\n",
       " 'classification': 2,\n",
       " 'and': 1,\n",
       " 'sentiment': 8,\n",
       " 'analysis': 0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  if you have the name of the term and you look for the column position of it at the tf-idf matrix then you go for \n",
    "# the \".vocabulary_\" method. \n",
    "vec1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 6, 'language': 5, 'processing': 7, 'this': 10, 'is': 4, 'used': 11, 'for': 3, 'text': 9, 'classification': 2, 'and': 1, 'sentiment': 8, 'analysis': 0}\n"
     ]
    }
   ],
   "source": [
    "print(vec1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = vec1.transform(tex1) #converting to sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28867513, 0.28867513, 0.28867513, 0.28867513, 0.28867513,\n",
       "        0.28867513, 0.28867513, 0.28867513, 0.28867513, 0.28867513,\n",
       "        0.28867513, 0.28867513]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "vector = vectorizer.transform([text[0]])\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analysis',\n",
       " 'and',\n",
       " 'classification',\n",
       " 'for',\n",
       " 'is',\n",
       " 'language',\n",
       " 'natural',\n",
       " 'processing',\n",
       " 'sentiment',\n",
       " 'text',\n",
       " 'this',\n",
       " 'used']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1.get_feature_names() # a function which gives the names of all the features in the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
